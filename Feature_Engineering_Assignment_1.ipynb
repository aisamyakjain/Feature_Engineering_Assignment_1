{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34c0b5af-24b9-42ed-927c-1a2f8a4dba58",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its \n",
    " application\n",
    " \n",
    " \n",
    " Answer: Min-Max scaling, also known as normalization, is a data preprocessing technique used to rescale numeric features within a specific range. The purpose of this scaling method is to bring all the features to a common scale without distorting the original distribution of the data.\n",
    "\n",
    "The Min-Max scaling formula is given as:\n",
    "\n",
    "\\[ X_{\\text{new}} = \\frac{{X - X_{\\text{min}}}}{{X_{\\text{max}} - X_{\\text{min}}}} \\]\n",
    "\n",
    "Where:\n",
    "- \\(X\\) is the original value of a feature.\n",
    "- \\(X_{\\text{min}}\\) is the minimum value of that feature in the dataset.\n",
    "- \\(X_{\\text{max}}\\) is the maximum value of that feature in the dataset.\n",
    "- \\(X_{\\text{new}}\\) is the rescaled value of the feature within the range of 0 to 1.\n",
    "\n",
    "By applying this formula, the minimum value in the dataset becomes 0, the maximum value becomes 1, and all other values are proportionally scaled in between. This technique is particularly useful when the range of features varies significantly and when algorithms are sensitive to the scale of the input data, such as gradient descent-based optimization algorithms.\n",
    "\n",
    "Here's an example to illustrate its application:\n",
    "\n",
    "Let's say we have a dataset of house prices with two features: \"Area\" and \"Number of Bedrooms.\" The \"Area\" feature has values ranging from 500 sq. ft. to 2000 sq. ft., while the \"Number of Bedrooms\" feature has values ranging from 1 to 5.\n",
    "\n",
    "Original data:\n",
    "```\n",
    "|   Area   | Bedrooms |\n",
    "|----------|----------|\n",
    "|   500    |    1     |\n",
    "|  1000    |    2     |\n",
    "|  1500    |    3     |\n",
    "|  2000    |    4     |\n",
    "```\n",
    "\n",
    "To apply Min-Max scaling, we calculate the minimum and maximum values of each feature:\n",
    "\n",
    "- For the \"Area\" feature: \\(X_{\\text{min}} = 500\\) and \\(X_{\\text{max}} = 2000\\).\n",
    "- For the \"Number of Bedrooms\" feature: \\(X_{\\text{min}} = 1\\) and \\(X_{\\text{max}} = 4\\).\n",
    "\n",
    "Applying the Min-Max scaling formula, we can rescale the features to the range of 0 to 1:\n",
    "\n",
    "```\n",
    "|   Area   | Bedrooms |\n",
    "|----------|----------|\n",
    "|   0.0    |   0.0    |\n",
    "|   0.333  |   0.333  |\n",
    "|   0.667  |   0.667  |\n",
    "|   1.0    |   1.0    |\n",
    "```\n",
    "\n",
    "After Min-Max scaling, both features are now within the range of 0 to 1, making them more comparable and suitable for feeding into machine learning algorithms that rely on scaled inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a170644-7e17-4e33-b08a-1aeccd21796b",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? \n",
    "Provide an example to illustrate its application\n",
    "\n",
    "Answer: The Unit Vector technique, also known as normalization or feature scaling, rescales features to have a unit norm (length of 1). It differs from Min-Max scaling as it focuses on the direction of the data points rather than their range. It is useful when the magnitude of features varies widely. For example, normalizing a dataset of vectors to unit norm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a74a8b0-3d47-4a73-8572-2755a398c9f0",
   "metadata": {},
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an \n",
    "example to illustrate its application.\n",
    "\n",
    "Answer: PCA (Principal Component Analysis) is a statistical technique used for dimensionality reduction. It transforms high-dimensional data into a lower-dimensional representation while preserving the most important information. It identifies the principal components (orthogonal directions of maximum variance) and projects the data onto these components. For example, reducing the dimensions of an image dataset while retaining the most relevant visual features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c839141-81da-43e4-9bc3-21165920898e",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature \n",
    "Extraction? Provide an example to illustrate this concept.\n",
    "\n",
    "Answer: PCA is a dimensionality reduction technique that can be used for feature extraction. It identifies the most informative features (principal components) in the data and discards the less important ones. These principal components can serve as new features, capturing the most significant information. For example, extracting facial features (like eyes, nose, and mouth) from images using PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c62c83-1a83-4501-90f5-d31097d3d57b",
   "metadata": {},
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset \n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to \n",
    "preprocess the data.\n",
    "\n",
    "Answer: To preprocess the data for the food delivery recommendation system, I would use Min-Max scaling. It would transform the numerical features like price, rating, and delivery time into a common range (e.g., 0 to 1) while preserving their relative relationships. This ensures that no single feature dominates the recommendation process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30ae5f4-d5d5-427c-b4ed-2ac05bedcd42",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many \n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the \n",
    "dimensionality of the dataset.\n",
    "\n",
    "Answer: To reduce the dimensionality of the stock price prediction dataset, I would apply PCA. It would identify the most important features that explain the maximum variance in the data. By selecting a subset of these principal components, we can effectively reduce the number of features while retaining crucial information for modeling stock price movements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e1dacb9a-ec2d-43ee-8b21-f82c5cc0c9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.0, -0.5789473684210527, -0.052631578947368474, 0.4736842105263157, 1.0]\n"
     ]
    }
   ],
   "source": [
    "#  Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the \n",
    "# values to a range of -1 to 1.\n",
    "\n",
    "\n",
    "data = [1, 5, 10, 15, 20]\n",
    "min_val = min(data)\n",
    "max_val = max(data)\n",
    "scaled_data = []\n",
    "\n",
    "for value in data:\n",
    "    scaled_value = (value - min_val) / (max_val - min_val) * 2 - 1\n",
    "    scaled_data.append(scaled_value)\n",
    "\n",
    "print(scaled_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9445f814-a8df-4bc4-bf4b-d98adc0ad3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of principal components to retain: 2\n"
     ]
    }
   ],
   "source": [
    "# Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform \n",
    "# Feature Extraction using PCA. How many principal components would you choose to retain, and why\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Sample dataset\n",
    "data = np.array([[170, 65, 30, 1, 120],\n",
    "                 [165, 60, 35, 0, 130],\n",
    "                 [180, 75, 40, 1, 140],\n",
    "                 [160, 55, 28, 0, 110]])\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA()\n",
    "pca.fit(data)\n",
    "\n",
    "# Determine the explained variance ratio of each principal component\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# Calculate the cumulative explained variance ratio\n",
    "cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# Choose the number of principal components to retain based on explained variance threshold\n",
    "n_components = np.sum(cumulative_variance_ratio < 0.95) + 1\n",
    "\n",
    "print(\"Number of principal components to retain:\", n_components)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a05eb6a-51dd-4e4a-aca5-b232bf4da340",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
